\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{biblatex}
\addbibresource{sample.bib}

\title{ECFP Transformations}

\author{Albert Dinstl}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
We present a comprehensive ablation study investigating the effects of various transformations on Extended-Connectivity Fingerprints (ECFPs) for molecular property prediction. Through 84 systematic experiments, we evaluate how affine transformations, noise injection, normalization techniques, and novel block radius linear mixing affect both distance preservation and downstream task performance. Our results demonstrate that count-based ECFPs exhibit greater robustness to noise than binary ECFPs, while most affine transformations (rotation, permutation, reflection, shear) preserve distances perfectly with minimal performance impact. We identify block radius linear mixing with ReLU activation as achieving the best classification performance (ROC-AUC = 0.8521 on BACE), while rotation of count ECFPs yields the lowest regression error (RMSE = 0.6775 on ESOL).
\end{abstract}

\section{Introduction}

Extended-Connectivity Fingerprints (ECFPs) have become a standard representation for molecular structures in cheminformatics and drug discovery \cite{rogers2010extended}. These fixed-length binary or count vectors encode local chemical environments and have proven effective for molecular similarity assessment and property prediction tasks. However, the effect of various mathematical transformations on ECFP representations remains underexplored.

The ultimate goal of this research is to design a Graph Neural Network (GNN) architecture that produces structurally equivalent molecular embeddings to ECFPs when frozen, essentially establishing ECFP's downstream task performance as a lower bound. Understanding how different transformations affect ECFP representations is a crucial step toward this goal, as it reveals which mathematical operations preserve the essential structural information encoded in these fingerprints.

In this work, we systematically investigate how different classes of transformations affect both (1) the preservation of pairwise distance relationships between molecular representations, and (2) performance on downstream molecular property prediction tasks. We evaluate 84 different experimental configurations across three diverse molecular datasets, encompassing regression and classification tasks.

\section{Methodology}

\subsection{Datasets}

We evaluate our transformations on three benchmark molecular property prediction datasets:

\begin{itemize}
    \item \textbf{ESOL (Aqueous Solubility):} Regression task predicting water solubility of compounds
    \item \textbf{LIPO (Lipophilicity):} Regression task predicting octanol-water partition coefficients
    \item \textbf{BACE:} Binary classification task predicting Beta-site APP Cleaving Enzyme 1 inhibition
\end{itemize}

\subsection{Fingerprint Representations}

We test two variants of ECFPs:
\begin{itemize}
    \item \textbf{Binary ECFPs:} Standard bit-vector representations where each bit indicates the presence/absence of a structural feature
    \item \textbf{Count ECFPs:} Integer-valued vectors encoding the frequency of each structural feature
\end{itemize}

For block radius mixing experiments, we use multi-radius representations that concatenate ECFP features computed at radii 0--3, capturing both local and extended chemical environments.

\subsection{Transformations Evaluated}

We organize our experiments into five categories:

\paragraph{Baselines:} Unmodified binary and count ECFPs establish baseline performance.

\paragraph{Affine Transformations:} Six geometric transformations applied to fingerprint vectors:
\begin{itemize}
    \item Rotation (orthogonal transformation)
    \item Permutation (coordinate reordering)
    \item Translation (additive shift)
    \item Shear (non-uniform scaling along axes)
    \item Reflection (coordinate sign flipping)
    \item Scaling (multiplicative factor)
\end{itemize}

\paragraph{Noise Injection:} Gaussian noise ($\sigma = 0.2$) added to fingerprint components to evaluate robustness.

\paragraph{Normalization:} Two standardization techniques:
\begin{itemize}
    \item L2 Normalization (unit-length vectors)
    \item Standardization (zero mean, unit variance)
\end{itemize}

\paragraph{Block Radius Linear Mixing:} Novel approach applying learnable linear transformations to multi-radius fingerprints with three activation functions:
\begin{itemize}
    \item Identity (linear combination)
    \item ReLU (rectified linear activation)
    \item Tanh (hyperbolic tangent activation)
\end{itemize}

\subsection{Evaluation Metrics}

\paragraph{Downstream Task Performance:}
\begin{itemize}
    \item Root Mean Squared Error (RMSE) for regression tasks (ESOL, LIPO)
    \item ROC Area Under Curve (ROC-AUC) for classification (BACE)
\end{itemize}

\paragraph{Distance Preservation:}
We compute Spearman rank correlations between pairwise distances in the original and transformed spaces for 300 randomly sampled molecules per dataset. Three distance metrics are evaluated:
\begin{itemize}
    \item Tanimoto distance (standard for binary fingerprints)
    \item Euclidean distance
    \item Cosine distance
\end{itemize}

Perfect preservation yields correlation = 1.0, while complete destruction yields correlation $\approx$ 0.

\section{Results}

\subsection{Baseline Performance}

Binary and count ECFP baselines establish the following performance levels:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Dataset} & \textbf{Binary ECFP} & \textbf{Count ECFP} \\
\hline
ESOL (RMSE $\downarrow$) & 1.0992 & \textbf{0.7606} \\
LIPO (RMSE $\downarrow$) & 0.8260 & \textbf{0.7302} \\
BACE (ROC-AUC $\uparrow$) & \textbf{0.8672} & 0.8545 \\
\hline
\end{tabular}
\caption{Baseline performance for binary and count ECFPs. Count ECFPs outperform binary ECFPs on regression tasks, while binary ECFPs show slight advantage on classification.}
\label{tab:baselines}
\end{table}

\subsection{Affine Transformations}

\paragraph{Rotation:} Achieves perfect distance preservation across all metrics (Tanimoto, Euclidean, Cosine $\geq$ 0.999) with the best overall regression performance. Count ECFP with rotation yields ESOL RMSE = 0.6775, the lowest error observed across all single-transformation experiments.

\paragraph{Permutation:} Exhibits perfect distance preservation (all correlations = 1.000) across all metrics and fingerprint types. Downstream performance remains comparable to baselines, with BACE Binary ROC-AUC = 0.8694. Recognized as the best transformation for distance preservation.

\paragraph{Translation:} Shows selective distance disruptionâ€”Euclidean distance is perfectly preserved (correlation = 1.000), while Tanimoto (0.24--0.99) and Cosine (0.23--0.99) distances are partially degraded. Despite distance disruption, downstream performance remains reasonable (ESOL Count RMSE = 0.8161).

\paragraph{Shear:} Maintains excellent distance preservation (Tanimoto = 0.9996--1.000) with strong downstream performance. BACE Binary achieves ROC-AUC = 0.8729, the best result among affine transformations for classification.

\paragraph{Reflection:} Achieves near-perfect or perfect distance preservation across all metrics. Performance closely matches rotation and permutation, confirming that coordinate sign flips do not meaningfully alter the fingerprint's predictive information.

\paragraph{Scaling:} Moderately degrades distance preservation (Tanimoto = 0.89--0.98 for binary, 0.95--0.98 for count) and downstream performance (ESOL Binary RMSE = 1.1338). Count fingerprints show better resilience to scaling than binary fingerprints.

\subsection{Noise Injection}

Gaussian noise ($\sigma = 0.2$) reveals differential robustness between fingerprint types:

\begin{itemize}
    \item \textbf{Binary ECFPs:} Severely impacted with Tanimoto correlation $\approx$ 0.90 and ESOL RMSE degrading to 1.7148 (55\% increase over baseline)
    \item \textbf{Count ECFPs:} Demonstrate robustness with Tanimoto $\approx$ 0.98--0.99 and ESOL RMSE = 0.9437 (24\% increase). BACE classification remains nearly unchanged (ROC-AUC = 0.8580)
\end{itemize}

\subsection{Normalization}

\paragraph{L2 Normalization:} Exhibits unique metric-specific behavior. Euclidean distance is severely disrupted (correlation $\approx$ 0.01--0.56) due to projection onto the unit hypersphere, while Cosine distance is perfectly preserved (correlation $\approx$ 0.999). Downstream performance remains comparable to baselines (ESOL Count RMSE = 0.9614).

\paragraph{Standardization:} Causes severe distance destruction across all metrics (Tanimoto = 0.26--0.71) and yields the worst downstream performance among normalization techniques (ESOL Binary RMSE = 1.3319, ESOL Count RMSE = 1.3906). Surprisingly, BACE classification remains stable (Count ROC-AUC = 0.8561), suggesting classification tasks are more robust to this transformation.

\subsection{Block Radius Linear Mixing}

Multi-radius baselines concatenating ECFP features at radii 0--3 already improve performance:

\begin{itemize}
    \item LIPO Count: RMSE = 0.6752 (best count-based result overall)
    \item BACE Binary: ROC-AUC = 0.8831 (best binary classification baseline)
\end{itemize}

Applying learnable linear transformations with different activations yields:

\paragraph{Identity Activation:} Linear mixing without nonlinearity achieves LIPO Count RMSE = 0.6661 and ESOL Binary RMSE = 1.0465.

\paragraph{ReLU Activation:} Produces the best overall results:
\begin{itemize}
    \item ESOL Binary: RMSE = 1.0163 (best binary regression performance)
    \item BACE Binary: ROC-AUC = 0.8521 (\textbf{best classification performance overall})
\end{itemize}

\paragraph{Tanh Activation:} Comparable to identity activation with BACE Binary ROC-AUC = 0.8724, demonstrating that different nonlinearities offer complementary performance trade-offs.

\subsection{Best Performing Configurations}

Across all 84 experiments, the top-performing configurations are:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\hline
\textbf{Dataset} & \textbf{Best Configuration} & \textbf{Score} \\
\hline
ESOL (RMSE $\downarrow$) & Count + Rotation & 0.6775 \\
LIPO (RMSE $\downarrow$) & Count + Block Mixing (Identity) & 0.6661 \\
BACE (ROC-AUC $\uparrow$) & Binary + Block Mixing (ReLU) & 0.8521 \\
\hline
\end{tabular}
\caption{Best performing experimental configurations per dataset.}
\label{tab:best_results}
\end{table}

\section{Discussion}

\subsection{Transformation Effects on Distance Preservation}

Our systematic evaluation reveals distinct patterns in how transformations affect distance preservation:

\paragraph{Perfect Preservation:} Rotation, permutation, reflection, and shear transformations preserve all three distance metrics nearly perfectly (correlations $\geq$ 0.999), confirming that these isometric transformations maintain the geometric structure of the fingerprint space.

\paragraph{Metric-Specific Effects:} L2 normalization demonstrates that transformations can selectively preserve certain metrics while destroying others. The perfect preservation of cosine distance but destruction of Euclidean distance reflects the mathematical properties of unit-sphere projection.

\paragraph{Degradation Patterns:} Scaling shows moderate degradation, standardization shows severe degradation, and noise injection shows differential effects based on fingerprint type. These patterns suggest a hierarchy of transformation "severity" in terms of structural information loss.

\subsection{Binary vs. Count ECFPs}

Count-based ECFPs consistently demonstrate superior robustness to noise (Tanimoto correlation 0.98--0.99 vs. 0.90 for binary) and better baseline regression performance. However, binary ECFPs show slight advantages in classification tasks and are less affected by standardization's pathological behavior on count distributions.

\subsection{Block Radius Linear Mixing}

The multi-radius approach with learnable mixing successfully improves upon single-radius baselines, particularly with ReLU activation for classification. This suggests that learnable combinations of features at different radii capture complementary chemical information, and that nonlinear activation functions can enhance representation quality for specific tasks.

\subsection{Implications for GNN Design}

These findings provide crucial guidance for designing GNN architectures that mimic ECFP behavior:

\begin{itemize}
    \item Permutation invariance (like graph neural networks naturally possess) aligns perfectly with ECFP properties, as permutation preserves all distances
    \item Rotation invariance should be incorporated where possible, as it maintains performance while enabling geometric flexibility
    \item Multi-scale aggregation (analogous to multi-radius ECFPs) with learnable mixing should be prioritized
    \item Count-based aggregation mechanisms may offer better robustness than binary indicators
\end{itemize}

\section{Conclusion}

Through this comprehensive ablation study of 84 experimental configurations across three molecular property prediction datasets, we have systematically characterized how various transformations affect ECFP representations. Our key findings include:

\begin{enumerate}
    \item Most affine transformations (rotation, permutation, reflection, shear) preserve distance relationships perfectly with minimal impact on downstream performance
    \item Count ECFPs exhibit significantly greater robustness to noise than binary ECFPs
    \item L2 normalization creates metric-specific preservation patterns, while standardization severely degrades both distances and performance
    \item Block radius linear mixing with learned transformations improves performance, with ReLU activation achieving the best classification results
    \item The best regression performance (ESOL RMSE = 0.6775) comes from count ECFP with rotation, while the best classification (BACE ROC-AUC = 0.8521) uses binary block mixing with ReLU
\end{enumerate}

These results establish a foundation for understanding ECFP transformation properties and provide empirical guidance for designing GNN architectures that leverage ECFP-like representations. Future work will focus on incorporating these insights into frozen GNN embeddings that match or exceed ECFP performance while maintaining interpretability and geometric properties.

\printbibliography

\end{document}